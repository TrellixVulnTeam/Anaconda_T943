
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass{article}

    
    
    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    

    
    
    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}
    
    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}
    
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Lecture-6B-HPC}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{figure}[htbp]
\centering
\includegraphics{./images/Continuum_Logo_0702.png}
\caption{Continuum Logo}
\end{figure}

    \section{Tools for high-performance computing
applications}\label{tools-for-high-performance-computing-applications}

    This curriculum builds on material by J. Robert Johansson from his
``Introduction to scientific computing with Python,'' generously made
available under a
\href{http://creativecommons.org/licenses/by/3.0/}{Creative Commons
Attribution 3.0 Unported License} at
https://github.com/jrjohansson/scientific-python-lectures. The Continuum
Analytics enhancements use the
\href{https://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons
Attribution-NonCommercial 4.0 International License}.

\begin{center}\rule{3in}{0.4pt}\end{center}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\end{Verbatim}

    \subsection{multiprocessing}\label{multiprocessing}

    Python has a built-in process-based library for concurrent computing,
called \texttt{multiprocessing}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{multiprocessing}
        \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{k+kn}{import} \PY{n+nn}{time}
        \PY{k+kn}{import} \PY{n+nn}{numpy}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{task}\PY{p}{(}\PY{n}{args}\PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{PID =}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{os}\PY{o}{.}\PY{n}{getpid}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{, args =}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{args}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{os}\PY{o}{.}\PY{n}{getpid}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{args}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{task}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{test}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
PID = 11411 , args = test
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} (11411, 'test')
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{pool} \PY{o}{=} \PY{n}{multiprocessing}\PY{o}{.}\PY{n}{Pool}\PY{p}{(}\PY{n}{processes}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{result} \PY{o}{=} \PY{n}{pool}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{task}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
PID = 11425 , args = 2
PID = 11426 , args = 3
PID = 11427 , args = 4
PID = 11424 , args = 1
PID = 11425 , args = 7
PID = 11426 , args = 6
PID = 11427 , args = 5
PID = 11427 , args = 8
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{result}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} [(11424, 1),
         (11425, 2),
         (11426, 3),
         (11427, 4),
         (11427, 5),
         (11426, 6),
         (11425, 7),
         (11427, 8)]
\end{Verbatim}
        
    The multiprocessing package is very useful for highly parallel tasks
that do not need to communicate with each other, other than when sending
the initial data to the pool of processes and when and collecting the
results.

    \subsection{IPython parallel}\label{ipython-parallel}

    IPython includes a very interesting and versatile parallel computing
environment, which is very easy to use. It builds on the concept of
ipython engines and controllers, that one can connect to and submit
tasks to. To get started using this framework for parallel computing,
one first have to start up an IPython cluster of engines. The easiest
way to do this is to use the \texttt{ipcluster} command,

\begin{verbatim}
$ ipcluster start -n 4
\end{verbatim}

Or, alternatively, from the ``Clusters'' tab on the IPython notebook
dashboard page. This will start 4 IPython engines on the current host,
which is useful for multicore systems. It is also possible to setup
IPython clusters that spans over many nodes in a computing cluster. For
more information about possible use cases, see the official
documentation \href{http://ipython.org/ipython-doc/dev/parallel/}{Using
IPython for parallel computing}.

To use the IPython cluster in our Python programs or notebooks, we start
by creating an instance of \texttt{IPython.parallel.Client}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{parallel} \PY{k}{import} \PY{n}{Client}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{cli} \PY{o}{=} \PY{n}{Client}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    Using the `ids' attribute we can retreive a list of ids for the IPython
engines in the cluster:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{cli}\PY{o}{.}\PY{n}{ids}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} [0, 1, 2, 3]
\end{Verbatim}
        
    Each of these engines are ready to execute tasks. We can selectively run
code on individual engines:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{def} \PY{n+nf}{getpid}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} return the unique ID of the current process \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k+kn}{import} \PY{n+nn}{os}
             \PY{k}{return} \PY{n}{os}\PY{o}{.}\PY{n}{getpid}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c}{\PYZsh{} first try it on the notebook process}
         \PY{n}{getpid}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} 11411
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c}{\PYZsh{} run it on one of the engines}
         \PY{n}{cli}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{apply\PYZus{}sync}\PY{p}{(}\PY{n}{getpid}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} 11464
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c}{\PYZsh{} run it on ALL of the engines at the same time}
         \PY{n}{cli}\PY{p}{[}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{apply\PYZus{}sync}\PY{p}{(}\PY{n}{getpid}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} [11464, 11478, 11477, 11479]
\end{Verbatim}
        
    We can use this cluster of IPython engines to execute tasks in parallel.
The easiest way to dispatch a function to different engines is to define
the function with the decorator:

\begin{verbatim}
@view.parallel(block=True)
\end{verbatim}

Here, \texttt{view} is supposed to be the engine pool which we want to
dispatch the function (task). Once our function is defined this way we
can dispatch it to the engine using the \texttt{map} method in the
resulting class (in Python, a decorator is a language construct which
automatically wraps the function into another function or a class).

To see how all this works, lets look at an example:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{dview} \PY{o}{=} \PY{n}{cli}\PY{p}{[}\PY{p}{:}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n+nd}{@dview}\PY{o}{.}\PY{n}{parallel}\PY{p}{(}\PY{n}{block}\PY{o}{=}\PY{k}{True}\PY{p}{)}
         \PY{k}{def} \PY{n+nf}{dummy\PYZus{}task}\PY{p}{(}\PY{n}{delay}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} a dummy task that takes \PYZsq{}delay\PYZsq{} seconds to finish \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k+kn}{import} \PY{n+nn}{os}\PY{o}{,} \PY{n+nn}{time}
         
             \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             \PY{n}{pid} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{getpid}\PY{p}{(}\PY{p}{)}
             \PY{n}{time}\PY{o}{.}\PY{n}{sleep}\PY{p}{(}\PY{n}{delay}\PY{p}{)}
             \PY{n}{t1} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             
             \PY{k}{return} \PY{p}{[}\PY{n}{pid}\PY{p}{,} \PY{n}{t0}\PY{p}{,} \PY{n}{t1}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c}{\PYZsh{} generate random delay times for dummy tasks}
         \PY{n}{delay\PYZus{}times} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}

    Now, to map the function \texttt{dummy\_task} to the random delay time
data, we use the \texttt{map} method in \texttt{dummy\_task}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{dummy\PYZus{}task}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{delay\PYZus{}times}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} [[11464, 1439847252.0649436, 1439847252.9168468],
          [11478, 1439847252.0697844, 1439847252.7821367],
          [11477, 1439847252.0755847, 1439847252.5272865],
          [11479, 1439847252.0843427, 1439847252.6486437]]
\end{Verbatim}
        
    Let's do the same thing again with many more tasks and visualize how
these tasks are executed on different IPython engines:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{def} \PY{n+nf}{visualize\PYZus{}tasks}\PY{p}{(}\PY{n}{results}\PY{p}{)}\PY{p}{:}
             \PY{n}{res} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{results}\PY{p}{)}
             \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{res}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{yticks} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{yticklabels} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{tmin} \PY{o}{=} \PY{n+nb}{min}\PY{p}{(}\PY{n}{res}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{k}{for} \PY{n}{n}\PY{p}{,} \PY{n}{pid} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{numpy}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{res}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{yticks}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{n}\PY{p}{)}
                 \PY{n}{yticklabels}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZpc{}}\PY{l+s}{d}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{pid}\PY{p}{)}
                 \PY{k}{for} \PY{n}{m} \PY{o+ow}{in} \PY{n}{numpy}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{res}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{n}{pid}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{:}
                     \PY{n}{ax}\PY{o}{.}\PY{n}{add\PYZus{}patch}\PY{p}{(}\PY{n}{plt}\PY{o}{.}\PY{n}{Rectangle}\PY{p}{(}\PY{p}{(}\PY{n}{res}\PY{p}{[}\PY{n}{m}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{tmin}\PY{p}{,} \PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.25}\PY{p}{)}\PY{p}{,}
                                  \PY{n}{res}\PY{p}{[}\PY{n}{m}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{res}\PY{p}{[}\PY{n}{m}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{green}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}
                 
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{o}{.}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n}\PY{o}{+}\PY{o}{.}\PY{l+m+mi}{5}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{max}\PY{p}{(}\PY{n}{res}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{tmin} \PY{o}{+} \PY{l+m+mf}{0.}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}yticks}\PY{p}{(}\PY{n}{yticks}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}yticklabels}\PY{p}{(}\PY{n}{yticklabels}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{PID}\PY{l+s}{\PYZdq{}}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{seconds}\PY{l+s}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{delay\PYZus{}times} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{result} \PY{o}{=} \PY{n}{dummy\PYZus{}task}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{delay\PYZus{}times}\PY{p}{)}
         \PY{n}{visualize\PYZus{}tasks}\PY{p}{(}\PY{n}{result}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Lecture-6B-HPC_files/Lecture-6B-HPC_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    That's a nice and easy parallelization! We can see that we utilize all
four engines quite well.

But one short coming so far is that the tasks are not load balanced, so
one engine might be idle while others still have more tasks to work on.

However, the IPython parallel environment provides a number of
alternative ``views'' of the engine cluster, and there is a view that
provides load balancing as well (above we have used the ``direct view'',
which is why we called it ``dview'').

To obtain a load balanced view we simply use the
\texttt{load\_balanced\_view} method in the engine cluster client
instance \texttt{cli}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{lbview} \PY{o}{=} \PY{n}{cli}\PY{o}{.}\PY{n}{load\PYZus{}balanced\PYZus{}view}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n+nd}{@lbview}\PY{o}{.}\PY{n}{parallel}\PY{p}{(}\PY{n}{block}\PY{o}{=}\PY{k}{True}\PY{p}{)}
         \PY{k}{def} \PY{n+nf}{dummy\PYZus{}task\PYZus{}load\PYZus{}balanced}\PY{p}{(}\PY{n}{delay}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} a dummy task that takes \PYZsq{}delay\PYZsq{} seconds to finish \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k+kn}{import} \PY{n+nn}{os}\PY{o}{,} \PY{n+nn}{time}
         
             \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             \PY{n}{pid} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{getpid}\PY{p}{(}\PY{p}{)}
             \PY{n}{time}\PY{o}{.}\PY{n}{sleep}\PY{p}{(}\PY{n}{delay}\PY{p}{)}
             \PY{n}{t1} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             
             \PY{k}{return} \PY{p}{[}\PY{n}{pid}\PY{p}{,} \PY{n}{t0}\PY{p}{,} \PY{n}{t1}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{result} \PY{o}{=} \PY{n}{dummy\PYZus{}task\PYZus{}load\PYZus{}balanced}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{delay\PYZus{}times}\PY{p}{)}
         \PY{n}{visualize\PYZus{}tasks}\PY{p}{(}\PY{n}{result}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Lecture-6B-HPC_files/Lecture-6B-HPC_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In the example above we can see that the engine cluster is a bit more
efficiently used, and the time to completion is shorter than in the
previous example.

    \subsubsection{Further reading}\label{further-reading}

    There are many other ways to use the IPython parallel environment. The
official documentation has a nice guide:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  http://ipython.org/ipython-doc/dev/parallel/
\end{itemize}

    \subsection{MPI}\label{mpi}

    When more communication between processes is required, sophisticated
solutions such as MPI and OpenMP are often needed. MPI is process based
parallel processing library/protocol, and can be used in Python programs
through the \texttt{mpi4py} package:

http://mpi4py.scipy.org/

To use the \texttt{mpi4py} package we include \texttt{MPI} from
\texttt{mpi4py}:

\begin{verbatim}
from mpi4py import MPI
\end{verbatim}

A MPI python program must be started using the \texttt{mpirun -n N}
command, where \texttt{N} is the number of processes that should be
included in the process group.

Note that the IPython parallel enviroment also has support for MPI, but
to begin with we will use \texttt{mpi4py} and the \texttt{mpirun} in the
follow examples.

    \subsubsection{Example 1}\label{example-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{file} mpitest.py
         
         from mpi4py import MPI
         
         comm = MPI.COMM\PYZus{}WORLD
         rank = comm.Get\PYZus{}rank()
         
         if rank == 0:
            data = [1.0, 2.0, 3.0, 4.0]
            comm.send(data, dest=1, tag=11)
         elif rank == 1:
            data = comm.recv(source=0, tag=11)
             
         print(\PYZdq{}rank =\PYZpc{}d, data =\PYZpc{}s\PYZdq{} \PYZpc{} (rank,data))
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Overwriting mpitest.py
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{o}{!}mpirun \PYZhy{}n \PY{l+m}{2} python mpitest.py
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
rank =0, data =[1.0, 2.0, 3.0, 4.0]
rank =1, data =[1.0, 2.0, 3.0, 4.0]
    \end{Verbatim}

    \subsubsection{Example 2}\label{example-2}

    Send a numpy array from one process to another:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{file} mpi\PYZhy{}numpy\PYZhy{}array.py
         
         from mpi4py import MPI
         import numpy
         
         comm = MPI.COMM\PYZus{}WORLD
         rank = comm.Get\PYZus{}rank()
         
         if rank == 0:
            data = numpy.random.rand(10)
            comm.Send(data, dest=1, tag=13)
         elif rank == 1:
            data = numpy.empty(10, dtype=numpy.float64)
            comm.Recv(data, source=0, tag=13)
             
         print(\PYZdq{}rank =\PYZpc{}d, data =\PYZpc{}s\PYZdq{} \PYZpc{} (rank,data))
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Overwriting mpi-numpy-array.py
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{o}{!}mpirun \PYZhy{}n \PY{l+m}{2} python mpi\PYZhy{}numpy\PYZhy{}array.py
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
rank =0, data =[ 0.36359812  0.55163426  0.02430927  0.91038764  0.79185896  0.20713193
  0.75367627  0.64960013  0.9828266   0.69653435]
rank =1, data =[ 0.36359812  0.55163426  0.02430927  0.91038764  0.79185896  0.20713193
  0.75367627  0.64960013  0.9828266   0.69653435]
    \end{Verbatim}

    \subsubsection{Example 3: Matrix-vector
multiplication}\label{example-3-matrix-vector-multiplication}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c}{\PYZsh{} prepare some random data}
         \PY{n}{N} \PY{o}{=} \PY{l+m+mi}{16}
         \PY{n}{A} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{N}\PY{p}{)}
         \PY{n}{numpy}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{random\PYZhy{}matrix.npy}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{A}\PY{p}{)}
         \PY{n}{x} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{)}
         \PY{n}{numpy}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{random\PYZhy{}vector.npy}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{x}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{file} mpi\PYZhy{}matrix\PYZhy{}vector.py
         
         from mpi4py import MPI
         import numpy
         
         comm = MPI.COMM\PYZus{}WORLD
         rank = comm.Get\PYZus{}rank()
         p = comm.Get\PYZus{}size()
         
         def matvec(comm, A, x):
             m = A.shape[0] / p
             y\PYZus{}part = numpy.dot(A[rank * m:(rank+1)*m], x)
             y = numpy.zeros\PYZus{}like(x)
             comm.Allgather([y\PYZus{}part,  MPI.DOUBLE], [y, MPI.DOUBLE])
             return y
         
         A = numpy.load(\PYZdq{}random\PYZhy{}matrix.npy\PYZdq{})
         x = numpy.load(\PYZdq{}random\PYZhy{}vector.npy\PYZdq{})
         y\PYZus{}mpi = matvec(comm, A, x)
         
         if rank == 0:
             y = numpy.dot(A, x)
             print(y\PYZus{}mpi)
             print(\PYZdq{}sum(y \PYZhy{} y\PYZus{}mpi) = \PYZpc{}f\PYZdq{} \PYZpc{} (y \PYZhy{} y\PYZus{}mpi).sum())
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Writing mpi-matrix-vector.py
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{o}{!}mpirun \PYZhy{}n \PY{l+m}{4} python mpi\PYZhy{}matrix\PYZhy{}vector.py
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[ 4.92983554  4.07883074  4.2398615   4.7207529   4.56465512  4.60331962
  5.40504466  3.35139743  3.70229     3.98073722  4.18009121  4.91292805
  3.35528324  4.48848917  3.87617524  4.02399674]
sum(y - y\_mpi) = 0.000000
    \end{Verbatim}

    \subsubsection{Example 4: Sum of the elements in a
vector}\label{example-4-sum-of-the-elements-in-a-vector}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c}{\PYZsh{} prepare some random data}
         \PY{n}{N} \PY{o}{=} \PY{l+m+mi}{128}
         \PY{n}{a} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{)}
         \PY{n}{numpy}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{random\PYZhy{}vector.npy}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{a}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{file} mpi\PYZhy{}psum.py
         
         from mpi4py import MPI
         import numpy as np
         
         def psum(a):
             r = MPI.COMM\PYZus{}WORLD.Get\PYZus{}rank()
             size = MPI.COMM\PYZus{}WORLD.Get\PYZus{}size()
             m = len(a) / size
             locsum = np.sum(a[r*m:(r+1)*m])
             rcvBuf = np.array(0.0, \PYZsq{}d\PYZsq{})
             MPI.COMM\PYZus{}WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM)
             return rcvBuf
         
         a = np.load(\PYZdq{}random\PYZhy{}vector.npy\PYZdq{})
         s = psum(a)
         
         if MPI.COMM\PYZus{}WORLD.Get\PYZus{}rank() == 0:
             print(\PYZdq{}sum = \PYZpc{}f, numpy sum = \PYZpc{}f\PYZdq{} \PYZpc{} (s,a.sum()))
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Overwriting mpi-psum.py
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{o}{!}mpirun \PYZhy{}n \PY{l+m}{4} python mpi\PYZhy{}psum.py
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
sum = 58.909508, numpy sum = 58.909508
    \end{Verbatim}

    \subsubsection{Further reading}\label{further-reading}

    \begin{itemize}
\item
  http://mpi4py.scipy.org
\item
  http://mpi4py.scipy.org/docs/usrman/tutorial.html
\item
  https://computing.llnl.gov/tutorials/mpi/
\end{itemize}

    \subsection{OpenMP}\label{openmp}

    What about OpenMP? OpenMP is a standard and widely used thread-based
parallel API that unfortunaltely is \textbf{not} useful directly in
Python. The reason is that the CPython implementation use a global
interpreter lock, making it impossible to simultaneously run several
Python threads. Threads are therefore not useful for parallel computing
in Python, unless it is only used to wrap compiled code that do the
OpenMP parallelization (Numpy can do something like that).

This is clearly a limitation in the Python interpreter, and as a
consequence all parallelization in Python must use processes (not
threads).

However, there is a way around this that is not that painful. When
calling out to compiled code the GIL is released, and it is possible to
write Python-like code in Cython where we can selectively release the
GIL and do OpenMP computations.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{N\PYZus{}core} \PY{o}{=} \PY{n}{multiprocessing}\PY{o}{.}\PY{n}{cpu\PYZus{}count}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{This system has }\PY{l+s}{\PYZpc{}}\PY{l+s}{d cores}\PY{l+s}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{N\PYZus{}core}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
This system has 2 cores
    \end{Verbatim}

    Here is a simple example that shows how OpenMP can be used via cython:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} Cython
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{cython} \PYZhy{}f \PYZhy{}c\PYZhy{}fopenmp \PYZhy{}\PYZhy{}link\PYZhy{}args=\PYZhy{}fopenmp \PYZhy{}c\PYZhy{}g
         
         cimport cython
         cimport numpy
         from cython.parallel import prange, parallel
         cimport openmp
         
         
         def cy\PYZus{}openmp\PYZus{}test():
             cdef int n, N
             \PYZsh{} release GIL so that we can use OpenMP
             with nogil, parallel():
                 N = openmp.omp\PYZus{}get\PYZus{}num\PYZus{}threads()
                 n = openmp.omp\PYZus{}get\PYZus{}thread\PYZus{}num()
                 with gil:
                     print(\PYZdq{}Number of threads \PYZpc{}d: thread number \PYZpc{}d\PYZdq{} \PYZpc{} (N, n))
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{cy\PYZus{}openmp\PYZus{}test}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of threads 2: thread number 0
Number of threads 2: thread number 1
    \end{Verbatim}

    \subsubsection{Example: matrix vector
multiplication}\label{example-matrix-vector-multiplication}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{c}{\PYZsh{} prepare some random data}
         \PY{n}{N} \PY{o}{=} \PY{l+m+mi}{4} \PY{o}{*} \PY{n}{N\PYZus{}core}
         
         \PY{n}{M} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{N}\PY{p}{)}
         \PY{n}{x} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}

    Let's first look at a simple implementation of matrix-vector
multiplication in Cython:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{cython}
         
         cimport cython
         cimport numpy
         import numpy
         
         @cython.boundscheck(False)
         @cython.wraparound(False)
         def cy\PYZus{}matvec(numpy.ndarray[numpy.float64\PYZus{}t, ndim=2] M, 
                       numpy.ndarray[numpy.float64\PYZus{}t, ndim=1] x, 
                       numpy.ndarray[numpy.float64\PYZus{}t, ndim=1] y):
         
             cdef int i, j, n = len(x)
         
             for i from 0 \PYZlt{}= i \PYZlt{} n:
                 for j from 0 \PYZlt{}= j \PYZlt{} n:
                     y[i] += M[i, j] * x[j]
                     
             return y
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{c}{\PYZsh{} check that we get the same results}
         \PY{n}{y} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{x}\PY{p}{)}
         \PY{n}{cy\PYZus{}matvec}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{numpy}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:} array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{o}{\PYZpc{}}\PY{k}{timeit} numpy.dot(M, x)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The slowest run took 465.61 times longer than the fastest. This could mean that an intermediate result is being cached 
1000000 loops, best of 3: 2.15 µs per loop
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{o}{\PYZpc{}}\PY{k}{timeit} cy\PYZus{}matvec(M, x, y)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The slowest run took 4.91 times longer than the fastest. This could mean that an intermediate result is being cached 
100000 loops, best of 3: 3.29 µs per loop
    \end{Verbatim}

    The Cython implementation here is a bit slower than numpy.dot, but not
by much, so if we can use multiple cores with OpenMP it should be
possible to beat the performance of numpy.dot.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{cython} \PYZhy{}f \PYZhy{}c\PYZhy{}fopenmp \PYZhy{}\PYZhy{}link\PYZhy{}args=\PYZhy{}fopenmp \PYZhy{}c\PYZhy{}g
         
         cimport cython
         cimport numpy
         from cython.parallel import parallel
         cimport openmp
         
         @cython.boundscheck(False)
         @cython.wraparound(False)
         def cy\PYZus{}matvec\PYZus{}omp(numpy.ndarray[numpy.float64\PYZus{}t, ndim=2] M, 
                           numpy.ndarray[numpy.float64\PYZus{}t, ndim=1] x, 
                           numpy.ndarray[numpy.float64\PYZus{}t, ndim=1] y):
         
             cdef int i, j, n = len(x), N, r, m
         
             \PYZsh{} release GIL, so that we can use OpenMP
             with nogil, parallel():
                 N = openmp.omp\PYZus{}get\PYZus{}num\PYZus{}threads()
                 r = openmp.omp\PYZus{}get\PYZus{}thread\PYZus{}num()
                 m = n / N
                 
                 for i from 0 \PYZlt{}= i \PYZlt{} m:
                     for j from 0 \PYZlt{}= j \PYZlt{} n:
                         y[r * m + i] += M[r * m + i, j] * x[j]
         
             return y
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{c}{\PYZsh{} check that we get the same results}
         \PY{n}{y} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{x}\PY{p}{)}
         \PY{n}{cy\PYZus{}matvec\PYZus{}omp}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{n}{numpy}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:} array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{o}{\PYZpc{}}\PY{k}{timeit} numpy.dot(M, x)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The slowest run took 297.37 times longer than the fastest. This could mean that an intermediate result is being cached 
1000000 loops, best of 3: 1.93 µs per loop
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{o}{\PYZpc{}}\PY{k}{timeit} cy\PYZus{}matvec\PYZus{}omp(M, x, y)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The slowest run took 69.79 times longer than the fastest. This could mean that an intermediate result is being cached 
100000 loops, best of 3: 10.3 µs per loop
    \end{Verbatim}

    Now, this implementation is much slower than numpy.dot for this problem
size, because of overhead associated with OpenMP and threading, etc. But
let's look at the how the different implementations compare with larger
matrix sizes:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n}{N\PYZus{}vec}  \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{25}\PY{p}{,} \PY{l+m+mi}{2000}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{)} \PY{o}{*} \PY{n}{N\PYZus{}core}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n}{duration\PYZus{}ref} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{)}\PY{p}{)}
         \PY{n}{duration\PYZus{}cy} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{)}\PY{p}{)}
         \PY{n}{duration\PYZus{}cy\PYZus{}omp} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{for} \PY{n}{idx}\PY{p}{,} \PY{n}{N} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{)}\PY{p}{:}
             
             \PY{n}{M} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{n}{N}\PY{p}{)}
             \PY{n}{x} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{N}\PY{p}{)}
             \PY{n}{y} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{x}\PY{p}{)}
             
             \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             \PY{n}{numpy}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{)}
             \PY{n}{duration\PYZus{}ref}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{t0}
             
             \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             \PY{n}{cy\PYZus{}matvec}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
             \PY{n}{duration\PYZus{}cy}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{t0}
             
             \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
             \PY{n}{cy\PYZus{}matvec\PYZus{}omp}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}
             \PY{n}{duration\PYZus{}cy\PYZus{}omp}\PY{p}{[}\PY{n}{idx}\PY{p}{]} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{t0}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{ax}\PY{o}{.}\PY{n}{loglog}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{,} \PY{n}{duration\PYZus{}ref}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{numpy}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{loglog}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{,} \PY{n}{duration\PYZus{}cy}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{cython}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{loglog}\PY{p}{(}\PY{n}{N\PYZus{}vec}\PY{p}{,} \PY{n}{duration\PYZus{}cy\PYZus{}omp}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{cython+openmp}\PY{l+s}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}yscale}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{log}\PY{l+s}{\PYZdq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{matrix\PYZhy{}vector multiplication duration}\PY{l+s}{\PYZdq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{matrix size}\PY{l+s}{\PYZdq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}59}]:} <matplotlib.text.Text at 0x7f536af122b0>
\end{Verbatim}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Lecture-6B-HPC_files/Lecture-6B-HPC_82_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For large problem sizes the the cython+OpenMP implementation is faster
than numpy.dot.

    With this simple implementation, the speedup for large problem sizes is
about:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{p}{(}\PY{p}{(}\PY{n}{duration\PYZus{}ref} \PY{o}{/} \PY{n}{duration\PYZus{}cy\PYZus{}omp}\PY{p}{)}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} 1.2483748994665467
\end{Verbatim}
        
    Obviously one could do a better job with more effort, since the
theoretical limit of the speed-up is:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{n}{N\PYZus{}core}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}61}]:} 2
\end{Verbatim}
        
    \subsubsection{Further reading}\label{further-reading}

    \begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  http://openmp.org
\item
  http://docs.cython.org/src/userguide/parallelism.html
\end{itemize}

    \subsection{OpenCL}\label{opencl}

    OpenCL is an API for heterogenous computing, for example using GPUs for
numerical computations. There is a python package called
\texttt{pyopencl} that allows OpenCL code to be compiled, loaded and
executed on the compute units completely from within Python. This is a
nice way to work with OpenCL, because the time-consuming computations
should be done on the compute units in compiled code, and in this Python
only server as a control language.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{file} opencl\PYZhy{}dense\PYZhy{}mv.py
        
        import pyopencl as cl
        import numpy
        import time
        
        \PYZsh{} problem size
        n = 10000
        
        \PYZsh{} platform
        platform\PYZus{}list = cl.get\PYZus{}platforms()
        platform = platform\PYZus{}list[0]
        
        \PYZsh{} device
        device\PYZus{}list = platform.get\PYZus{}devices()
        device = device\PYZus{}list[0]
        
        if False:
            print(\PYZdq{}Platform name:\PYZdq{} + platform.name)
            print(\PYZdq{}Platform version:\PYZdq{} + platform.version)
            print(\PYZdq{}Device name:\PYZdq{} + device.name)
            print(\PYZdq{}Device type:\PYZdq{} + cl.device\PYZus{}type.to\PYZus{}string(device.type))
            print(\PYZdq{}Device memory: \PYZdq{} + str(device.global\PYZus{}mem\PYZus{}size//1024//1024) + \PYZsq{} MB\PYZsq{})
            print(\PYZdq{}Device max clock speed:\PYZdq{} + str(device.max\PYZus{}clock\PYZus{}frequency) + \PYZsq{} MHz\PYZsq{})
            print(\PYZdq{}Device compute units:\PYZdq{} + str(device.max\PYZus{}compute\PYZus{}units))
        
        \PYZsh{} context
        ctx = cl.Context([device]) \PYZsh{} or we can use cl.create\PYZus{}some\PYZus{}context()
        
        \PYZsh{} command queue
        queue = cl.CommandQueue(ctx)
        
        \PYZsh{} kernel
        KERNEL\PYZus{}CODE = \PYZdq{}\PYZdq{}\PYZdq{}
        //
        // Matrix\PYZhy{}vector multiplication: r = m * v
        //
        \PYZsh{}define N \PYZpc{}(mat\PYZus{}size)d
        \PYZus{}\PYZus{}kernel
        void dmv\PYZus{}cl(\PYZus{}\PYZus{}global float *m, \PYZus{}\PYZus{}global float *v, \PYZus{}\PYZus{}global float *r)
        \PYZob{}
            int i, gid = get\PYZus{}global\PYZus{}id(0);
            
            r[gid] = 0;
            for (i = 0; i \PYZlt{} N; i++)
            \PYZob{}
                r[gid] += m[gid * N + i] * v[i];
            \PYZcb{}
        \PYZcb{}
        \PYZdq{}\PYZdq{}\PYZdq{}
        
        kernel\PYZus{}params = \PYZob{}\PYZdq{}mat\PYZus{}size\PYZdq{}: n\PYZcb{}
        program = cl.Program(ctx, KERNEL\PYZus{}CODE \PYZpc{} kernel\PYZus{}params).build()
        
        \PYZsh{} data
        A = numpy.random.rand(n, n)
        x = numpy.random.rand(n, 1)
        
        \PYZsh{} host buffers
        h\PYZus{}y = numpy.empty(numpy.shape(x)).astype(numpy.float32)
        h\PYZus{}A = numpy.real(A).astype(numpy.float32)
        h\PYZus{}x = numpy.real(x).astype(numpy.float32)
        
        \PYZsh{} device buffers
        mf = cl.mem\PYZus{}flags
        d\PYZus{}A\PYZus{}buf = cl.Buffer(ctx, mf.READ\PYZus{}ONLY | mf.COPY\PYZus{}HOST\PYZus{}PTR, hostbuf=h\PYZus{}A)
        d\PYZus{}x\PYZus{}buf = cl.Buffer(ctx, mf.READ\PYZus{}ONLY | mf.COPY\PYZus{}HOST\PYZus{}PTR, hostbuf=h\PYZus{}x)
        d\PYZus{}y\PYZus{}buf = cl.Buffer(ctx, mf.WRITE\PYZus{}ONLY, size=h\PYZus{}y.nbytes)
        
        \PYZsh{} execute OpenCL code
        t0 = time.time()
        event = program.dmv\PYZus{}cl(queue, h\PYZus{}y.shape, None, d\PYZus{}A\PYZus{}buf, d\PYZus{}x\PYZus{}buf, d\PYZus{}y\PYZus{}buf)
        event.wait()
        cl.enqueue\PYZus{}copy(queue, h\PYZus{}y, d\PYZus{}y\PYZus{}buf)
        t1 = time.time()
        
        print(\PYZdq{}opencl elapsed time =\PYZdq{}, (t1\PYZhy{}t0))
        
        \PYZsh{} Same calculation with numpy
        t0 = time.time()
        y = numpy.dot(h\PYZus{}A, h\PYZus{}x)
        t1 = time.time()
        
        print(\PYZdq{}numpy elapsed time =\PYZdq{}, (t1\PYZhy{}t0))
        
        \PYZsh{} see if the results are the same
        print(\PYZdq{}max deviation =\PYZdq{}, numpy.abs(y\PYZhy{}h\PYZus{}y).max())
\end{Verbatim}

    \begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{%} \NormalTok{python opencl-dense-mv.py}
\KeywordTok{(}\StringTok{'opencl elapsed time ='}\NormalTok{, }\KeywordTok{0.05729985237121582)}
\KeywordTok{(}\StringTok{'numpy elapsed time ='}\NormalTok{, }\KeywordTok{0.023556947708129883)}
\KeywordTok{(}\StringTok{'max deviation ='}\NormalTok{, }\KeywordTok{0.015380859)}
\end{Highlighting}
\end{Shaded}

    \subsubsection{Further reading}\label{further-reading}

    \begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  http://mathema.tician.de/software/pyopencl
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
